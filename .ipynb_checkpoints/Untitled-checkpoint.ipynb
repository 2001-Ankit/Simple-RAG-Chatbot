{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c078e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ankit Rai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ankit Rai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebbf00bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from a PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86ef81cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean extracted text\n",
    "def clean_text(text):\n",
    "    # Remove newlines and unnecessary whitespace\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^\\w\\s,.]', '', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5770d085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into smaller chunks\n",
    "def split_text(text, max_length=512):\n",
    "    sentences = text.split('. ')\n",
    "    chunks, chunk = [], []\n",
    "    for sentence in sentences:\n",
    "        if len(' '.join(chunk)) + len(sentence) <= max_length:\n",
    "            chunk.append(sentence)\n",
    "        else:\n",
    "            chunks.append(' '.join(chunk))\n",
    "            chunk = [sentence]\n",
    "    if chunk:\n",
    "        chunks.append(' '.join(chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02f43d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the PDF\n",
    "pdf_path = \"./History_Ancient_Medieval_Nepal.pdf\"\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "cleaned_text = clean_text(pdf_text)\n",
    "text_chunks = split_text(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bae9f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SentenceTransformer for embeddings\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Create embeddings for each text chunk\n",
    "embeddings = embedding_model.encode(text_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3469f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(embeddings.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ca88471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search for the nearest chunks in FAISS\n",
    "def search_in_faiss(query, top_k=5):\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    distances, indices = index.search(np.array(query_embedding), top_k)\n",
    "    return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdca6899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "   template = \"\"\"\n",
    "You are an assistant having the knowledge of the given PDF context, \n",
    "and you will try to answer as possible to the given context of the PDF. \n",
    "If you don't know the answer, don't try to make it up yourself.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer: \n",
    "\"\"\"\n",
    ",input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f57624d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "class SimpleHuggingFaceLLM:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.pipeline = pipeline(\"text-generation\", model=model_name)\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        result = self.pipeline(prompt, max_length=100, num_return_sequences=1)\n",
    "        return result[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9298080",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\"HuggingFaceLLM\" object has no field \"model_name\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize the Hugging Face model wrapper\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistilbert-base-cased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create the LLMChain with the prompt template and the LLM\u001b[39;00m\n\u001b[0;32m      5\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m LLMChain(prompt_template\u001b[38;5;241m=\u001b[39mprompt_template, llm\u001b[38;5;241m=\u001b[39mllm)\n",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36mHuggingFaceLLM.__init__\u001b[1;34m(self, model_name)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m model_name\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipeline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pydantic\\main.py:881\u001b[0m, in \u001b[0;36mBaseModel.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_validator__\u001b[38;5;241m.\u001b[39mvalidate_assignment(\u001b[38;5;28mself\u001b[39m, name, value)\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextra\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_fields:\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;66;03m# TODO - matching error\u001b[39;00m\n\u001b[1;32m--> 881\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m object has no field \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextra\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_fields:\n\u001b[0;32m    883\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_extra \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_extra:\n",
      "\u001b[1;31mValueError\u001b[0m: \"HuggingFaceLLM\" object has no field \"model_name\""
     ]
    }
   ],
   "source": [
    "# Initialize the custom Hugging Face model wrapper\n",
    "llm = SimpleHuggingFaceLLM(\"distilbert-base-cased\")\n",
    "\n",
    "# Define a function to use the custom LLM for generating answers\n",
    "def llm_answer(context: str, question: str) -> str:\n",
    "    prompt = prompt_template.render(context=context, question=question)\n",
    "    return llm.generate(prompt)\n",
    "\n",
    "# Create the LLMChain with the prompt template and the LLM\n",
    "llm_chain = LLMChain(prompt_template=prompt_template, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e01357c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the Hugging Face LLM\n",
    "# llm_chain = LLMChain(prompt_template=prompt_template, llm=pipeline(\"text-generation\", model=\"distilbert-base-cased\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0a5f841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Hugging Face QA pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "# Function to answer a question using QA model\n",
    "def answer_question(question, context):\n",
    "    return qa_pipeline(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d572f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatbot loop to engage in conversation\n",
    "conversation_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edbab94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_loop():\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "\n",
    "        # Retrieve relevant chunks based on the user query\n",
    "        search_indices = search_in_faiss(user_input)\n",
    "        relevant_chunks = [text_chunks[i] for i in search_indices[0]]\n",
    "\n",
    "        # Generate an answer using the QA model\n",
    "        answer = answer_question(user_input, ' '.join(relevant_chunks))\n",
    "\n",
    "        # Print the response\n",
    "        print(\"Chatbot:\", answer['answer'])\n",
    "\n",
    "        # Update conversation history\n",
    "        conversation_history.append({\"question\": user_input, \"answer\": answer['answer']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51a74cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: What is Nepal\n",
      "Chatbot: Tibet\n",
      "You: Who is the king of nepal\n",
      "Chatbot: Gupta\n",
      "You: Who is the King of Nepal\n",
      "Chatbot: Rajyabati\n",
      "You: stop\n",
      "Chatbot: 6g69 Ranipokhari 038rft\n",
      "You: quit\n"
     ]
    }
   ],
   "source": [
    "# Start chatbot loop\n",
    "chatbot_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15159a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd42480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
